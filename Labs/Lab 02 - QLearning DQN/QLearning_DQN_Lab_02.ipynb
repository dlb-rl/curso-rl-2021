{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QLearning DQN - Lab 02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1Yzpr-vXtPf"
      },
      "source": [
        "#Q-Learning & Deep Q Network - Lab 02\n",
        "\n",
        "\n",
        "Nesse laboratório iremos abordar sobre os temas: OpenAI Gym, Q-learning e DQN. Você irá encontrar descrições e referências sobre os temas abordados. \n",
        "\n",
        "Atente-se em encontrar os `# HW` nos códigos! Eles irão definir as tarefas que você deverá realizar afim de completar o código.\n",
        "\n",
        "Quaisquer dúvidas que tiverem, procurem mandar no canal `# dúvidas-labs` no Discord. Lembrando que às quintas-feiras a partir das 19h estaremos no canal `Sala-de-estudo` para tirar suas dúvidas e auxiliar em eventuais erros de execução no notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmZTlt6Jj2Fa"
      },
      "source": [
        "## Configuração "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQPQqlPXssFY"
      },
      "source": [
        "Você precisará fazer uma cópia deste notebook em seu Google Drive antes de editar. Você pode fazer isso com **Arquivo → Salvar uma cópia no Drive**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpR_VDEGaKo6"
      },
      "source": [
        "from collections import namedtuple, deque\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_32fm32aLOg"
      },
      "source": [
        "# Seu trabalho será armazenado em uma pasta chamada `minicurso_rl` por padrão \n",
        "# para evitar que o tempo limite da instância do Colab exclua suas edições\n",
        "\n",
        "\n",
        "DRIVE_PATH = \"/content/gdrive/My\\ Drive/minicurso_rl/lab02\"\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace(\"\\\\\", \"\")\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "  %mkdir $DRIVE_PATH\n",
        "\n",
        "SYM_PATH = \"/content/minicurso_rl\"\n",
        "if not os.path.exists(SYM_PATH):\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFwQD_0raOeo"
      },
      "source": [
        "!pip install \"gym[all]\"  > /dev/null 2>&1 \n",
        "!pip install \"gym[box2d]\"  > /dev/null 2>&1 \n",
        "!pip install box2d-py > /dev/null 2>&1 \n",
        "\n",
        "!apt-get install x11-utils > /dev/null 2>&1 \n",
        "!pip install pyglet > /dev/null 2>&1 \n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "!pip install plotly > /dev/null 2>&1\n",
        "!pip install --upgrade ceia-soccer-twos > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DmbNS0Mx8Uy"
      },
      "source": [
        "! wget http://www.atarimania.com/roms/Roms.rar\n",
        "! mkdir /content/ROM/\n",
        "! unrar e /content/Roms.rar /content/ROM/ -y\n",
        "! python -m atari_py.import_roms /content/ROM/ > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DodV33PaQPa"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "torch.manual_seed(10)\n",
        "random.seed(10)\n",
        "np.random.seed(10)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICy4G88WXep_"
      },
      "source": [
        "## OpenAI GYM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fudq-8pRsvuq"
      },
      "source": [
        "O [OpenAI Gym](https://gym.openai.com/) visa fornecer um benchmark fácil de configurar com uma ampla variedade de ambientes diferentes. O objetivo é padronizar como os ambientes são definidos nas publicações de pesquisa de IA para que as pesquisas publicadas se tornem mais facilmente reprodutíveis. O projeto pretende fornecer ao usuário uma interface simples.\n",
        "\n",
        "\n",
        "A peça central do Gym é o ambiente, que define o \"jogo\" em que seu algoritmo de reforço competirá. Um ambiente não precisa ser um jogo; no entanto, ele descreve os seguintes recursos semelhantes aos de um jogo:\n",
        "\n",
        "- **Espaço de ação**: Quais ações podemos realizar no ambiente, a cada etapa / episódio, para alterar o ambiente.\n",
        "- **Espaço de observação**: Qual é o estado atual da parte do ambiente que podemos observar. Normalmente, podemos ver todo o ambiente.\n",
        "\n",
        "\n",
        "Antes de começarmos a examinar o Gym, é essencial entender um pouco da terminologia usada por esta biblioteca.\n",
        "\n",
        "- **Agent** - o programa ou modelo de aprendizado de máquina que controla as ações. \n",
        "- **Step** - Uma rodada de ações que afetam o estado do ambiente.\n",
        "- **Episode** - Uma coleção de steps que termina quando o agente cumpre ou falha em cumprir o objetivo do ambiente ou o episódio atinge o número máximo de etapas permitidas.\n",
        "- **Reward** - Reforço positivo/negativo que pode ocorrer ao final de cada episódio ou após cada ação do agente.\n",
        "- **Render** - O Gym pode renderizar um quadro para exibição após cada episódio.\n",
        "- **Nondeterministic** - para alguns ambientes, a aleatoriedade é um fator na decisão de quais efeitos as ações têm na recompensa e nas mudanças no espaço de observação.\n",
        "\n",
        "\n",
        "A biblioteca Gym nos permite consultar alguns desses atributos de ambientes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miXXkXcDsZNd"
      },
      "source": [
        "import gym\n",
        "\n",
        "def query_environment(name):\n",
        "    \n",
        "  env = gym.make(name)\n",
        "  spec = gym.spec(name)\n",
        "\n",
        "  print(f\"Action Space: {env.action_space}\")\n",
        "  print(f\"Observation Space: {env.observation_space}\")\n",
        "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "  print(f\"Reward Range: {env.reward_range}\")\n",
        "  print(f\"Reward Threshold: {spec.reward_threshold}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omRdcyHrsgCg"
      },
      "source": [
        "Começaremos examinando o ambiente `MountainCar-v0`, que desafia um carro a escapar do vale entre duas montanhas. \n",
        "\n",
        "\n",
        "Existem três ações distintas que podem ser tomadas: acelerar para frente, desacelerar ou acelerar para trás. O espaço de observação contém dois valores contínuos (ponto flutuante), conforme evidenciado pelo objeto Box. O espaço de observação é simplesmente a posição e a velocidade do carro. O carro tem 200 steps para escapar do vale a cada episódio. Você teria que olhar o código para saber, mas o carro não recebe qualquer recompensa incremental. A única recompensa é dada quando o carro foge do vale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfzHNzlzsbix"
      },
      "source": [
        "query_environment(\"MountainCar-v0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUeq_d3WtVt2"
      },
      "source": [
        "O ambiente `CartPole-v1` desafia o agente a mover um carrinho sobre um trilho enquanto mantém um mastro equilibrado. O ambiente possui um espaço de observação de 4 números contínuos:\n",
        "\n",
        "- Posição do carrinho\n",
        "- Velocidade do carrinho\n",
        "- Ângulo do mastro\n",
        "- Velocidade do mastro na ponta\n",
        "\n",
        "Para atingir esse objetivo, o agente pode realizar as seguintes ações:\n",
        "\n",
        "- Empurre o carrinho para a esquerda\n",
        "- Empurre o carrinho para a direita\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iQLfxbisoB0"
      },
      "source": [
        "query_environment(\"CartPole-v1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRRDcXXStfc7"
      },
      "source": [
        "Também existe uma variante contínua do carro de montanha. Esta versão não possui simplesmente o motor ligado ou desligado. Para o carro contínuo, o espaço de ação é um único número de ponto flutuante que especifica quanta força para frente ou para trás está sendo aplicada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRli8vwKsvAJ"
      },
      "source": [
        "query_environment(\"MountainCarContinuous-v0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x6C9tGiuHvo"
      },
      "source": [
        "Os jogos do Atari, como o Breakout, podem usar um espaço de observação que é igual ao tamanho da tela do Atari (210x160) ou até mesmo usar a memória RAM do Atari (128 bytes) para determinar o estado do jogo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIGS73eYsvZs"
      },
      "source": [
        "query_environment(\"Breakout-v0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyZwdTaHsx3B"
      },
      "source": [
        "query_environment(\"Breakout-ram-v0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o4HGZdHG4Ss"
      },
      "source": [
        "Você pode verificar outras classes de ambientes disponíveis como Algorithmic, Atari, Box2D e Robotics [aqui](https://gym.openai.com/envs/#algorithmic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz-3sgBYHVh0"
      },
      "source": [
        "Quando o agente interage com o ambiente através de uma ação, a função `step(action)` retorna uma tupla contendo a observação do próximo estado do ambiente, um sinal contínuo de recompensa, um sinal booleano de finalização de episódio (*done*), e um dicionário com informações extras para depuração.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1TXdjYkbfm2EvtCbVIpe5BkUgXJY1d1zE' width=\"600\" height=\"250\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKQ019EEHkz6"
      },
      "source": [
        "env = gym.make('MountainCarContinuous-v0') # tente para ambientes diferentes\n",
        "observation = env.reset()\n",
        "\n",
        "for t in range(3):\n",
        "    print(\"\\n----------\")\n",
        "    print(\"Observation: \", observation)\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    print(\"Next Observation: \", observation)\n",
        "    print(\"Reward: \", reward)\n",
        "    print(\"Done: \", done)\n",
        "    print(\"Info: \", info)\n",
        "    if done:\n",
        "        print(\"Finished after {} timesteps\".format(t+1))\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH2-Fu5UXe1A"
      },
      "source": [
        "# Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFZU8Hiis2U8"
      },
      "source": [
        "O Q-Learning é uma técnica fundamental de aprendizagem por reforço que é capaz de encontrar uma política ótima para um Processo de Decisão de Markov Finito, dado um número suficiente de iterações com o ambiente. O Q-Learning funciona construindo uma tabela que relaciona o valor de uma ação para cada estado possível."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZVWeRerkB1h"
      },
      "source": [
        "### Ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg-MI8ZutAwL"
      },
      "source": [
        "O ambiente utilizado será o [Taxi-v3](https://gym.openai.com/envs/Taxi-v3/).\n",
        "\n",
        "Existem 4 locais (R, G, Y, B) e o nosso objetivo é que o táxi pegue o passageiro no local certo e o desembarque no local correto. Para garantir que nosso agente se comporte dessa forma, temos uma recompensa de -1 para cada ação e uma recompensa adicional de +20 por entregar o passageiro. Há também uma recompensa de -10 por executar ações de “coleta” e “entrega” ilegalmente.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Ações\n",
        "\n",
        "Existem 6 ações determinísticas discretas:\n",
        "- 0 = mover para o sul (south)\n",
        "- 1 = mover para o norte (north)\n",
        "- 2 = mover para o leste (east)\n",
        "- 3 = mover para o oeste (west)\n",
        "- 4 = embarque de passageiro (pickup)\n",
        "- 5 = desembarque de passageiro (dropoff)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Observações\n",
        "\n",
        "A observação é dada por uma grade 5x5 que representa o mapa e a posição dos objetos no mesmo.\n",
        "\n",
        "<br>\n",
        "\n",
        "Locais de passageiros:\n",
        "\n",
        "[R]ed = 0, [Y]ellow = 1, [G]reen = 2, [B]lue = 3, Passageiro no táxi (o táxi fica com a cor verde) = 4.\n",
        "\n",
        "<br>\n",
        "\n",
        "Locais de entrega:\n",
        "\n",
        "[R]ed = 0, [Y]ellow = 1, [G]reen = 2, [B]lue = 3. \n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "Há quatro (4) destinos e cinco (4 + 1) locais de passageiros. Então, existem 5 x 5 x 4 x 5 = 500 estados discretos possíveis.\n",
        "\n",
        "\n",
        "Também podemos ver que uma letra azul indica a localização atual do passageiro, a cor rosa indica o local de desembarque, o táxi é amarelo e fica verde quando há um passageiro nele, '|' indica as paredes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nu0Veyrnk_7"
      },
      "source": [
        "# Procedimento para renderizar o ambiente \n",
        "\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0325pI8jXskG"
      },
      "source": [
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfT28k_iXqvu"
      },
      "source": [
        "environment_id = \"Taxi-v3\"\n",
        "env = gym.make(environment_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWUBj-F4y8OO"
      },
      "source": [
        "query_environment(environment_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o-V-eU6ab3h"
      },
      "source": [
        "frames = []\n",
        "episodes = 1\n",
        "for episode in range(1, episodes+1):\n",
        "    obs = env.reset()     # Retorna a observação inicial\n",
        "    done = False\n",
        "    score = 0\n",
        "    while not done:\n",
        "        action = env.action_space.sample()            # Seleciona uma ação aleatória\n",
        "        obs, reward, done, info = env.step(action)    # Executa a ação selecionada\n",
        "        score += reward\n",
        "\n",
        "        frames.append({\n",
        "            \"frame\": env.render(mode=\"ansi\"),\n",
        "            \"state\": obs,\n",
        "            \"action\": action,\n",
        "            \"reward\": reward\n",
        "            }\n",
        "        )\n",
        "    print(\"\\n\\nEpisódio: {} Pontuação: {}\".format(episode,score))\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnSScemTns1N"
      },
      "source": [
        "print_frames(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbLw07p1klAq"
      },
      "source": [
        "### Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP9Pu-CStJuY"
      },
      "source": [
        "Essencialmente, o Q-learning permite que o agente use as recompensas do ambiente para aprender, com o tempo, a melhor ação a ser executada em um determinado estado.\n",
        "\n",
        "Agora, inicializamos a tabela Q como um dicionário que armazena o par estado-ação especificando o valor de realizar uma ação *a* no estado *s*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ez_EsFuXrFS"
      },
      "source": [
        "q = {}\n",
        "for s in range(env.observation_space.n):\n",
        "    for a in range(env.action_space.n):\n",
        "        q[(s,a)] = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDc3py6Bkpzl"
      },
      "source": [
        "Um valor Q para uma determinada combinação de estado-ação é representativo da \"qualidade\" de uma ação realizada a partir desse estado. Melhores valores de Q implicam em melhores chances de obter maiores recompensas.\n",
        "\n",
        "Os valores Q são inicializados com um valor arbitrário e, conforme o agente se expõe ao ambiente e recebe diferentes recompensas ao executar ações diferentes, os valores Q são atualizados usando a equação:\n",
        "\n",
        "<br>\n",
        "\n",
        "$Q(state, action) \\leftarrow (1 - \\alpha) Q(state, action) + \\alpha(reward + \\gamma \\max\\limits_{a} Q(nextState, allActions))$\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Definimos uma função chamada `update_q_table` que irá atualizar os valores Q de acordo com a regra de atualização do Q-Learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHEpmUWAk11k"
      },
      "source": [
        "# HW construa a função update_q_table seguindo a equação mostrada acima\n",
        "def update_q_table(prev_state, action, reward, nextstate, alpha, gamma):\n",
        "    # Calcule max_a Q(next_state, all_actions) procurando na tabela Q quais \n",
        "    # ações para o estado next_state possuem o maior valor Q\n",
        "\n",
        "    # Atualize a tabela Q na posição (prev_state, action) conforme a regra de atualização\n",
        "    # (1-alpha) * Q(prev_state, action) + alpha(reward + gamma * max_a Q(next_state, all_actions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJcVVPQ1k7Np"
      },
      "source": [
        "\n",
        "Em seguida, definimos uma função para executar a política epsilon-greedy. Na política epsilon-greedy, selecionamos a melhor ação com probabilidade `1 - epsilon` ou exploramos uma nova ação (aleatória) com probabilidade `epsilon`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv_Qzv-FlH_A"
      },
      "source": [
        "# HW construa a função epsilon_greedy_policy seguindo a descrição acima\n",
        "def epsilon_greedy_policy(state, epsilon):\n",
        "    # Você pode utilizar a função random.uniform para amostrar um valor \n",
        "    # aleatório entre 0 e 1. Caso o valor amostrado seja menor do que epsilon\n",
        "    # retorne uma ação aleatória. Caso contrário retorne a ação que maximiza o \n",
        "    # valor Q para o estado state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYKFxTkClMa7"
      },
      "source": [
        "Agora inicializamos as variáveis necessárias\n",
        "\n",
        "`alpha` - taxa de aprendizagem ($0 < \\alpha \\leq 1$)\n",
        "\n",
        "`gama` - fator de desconto ($0 < \\gamma \\leq 1$)\n",
        "\n",
        "`epsilon` - valor epsilon na política epsilon-greedy ($0 < \\epsilon < 1$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRV-DUq7lUsl"
      },
      "source": [
        "alpha = 0.7\n",
        "gamma = 0.6\n",
        "epsilon = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idlPUUc0lbHL"
      },
      "source": [
        "Agora, vamos realizar o Q-Learning !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLJfUqjelcPc"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "epochs = 8000\n",
        "all_rewards = []\n",
        "for i in range(epochs):\n",
        "    r = 0\n",
        "    done = False\n",
        "\n",
        "    prev_state = env.reset()\n",
        "    \n",
        "    while not done:\n",
        "                \n",
        "        # HW: selecione a ação pela política epsilon-greedy\n",
        "        \n",
        "        # HW: Realize a ação no ambiente e receba o próximo estado, recompensa, done e info\n",
        "        \n",
        "        # HW: Atualize o valor Q usando a função update_q_table        \n",
        "        \n",
        "        # HW: Atualize o estado anterior como próximo estado\n",
        "\n",
        "        # Armazene todas as recompensas obtidas\n",
        "        r += reward\n",
        "\n",
        "    all_rewards.append(r)\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "        print(\"total reward: \", r)\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFPWRtU9A3et"
      },
      "source": [
        "import plotly.graph_objs as go\n",
        "\n",
        "fig = go.Figure([\n",
        "    go.Scatter(\n",
        "        y=all_rewards,\n",
        "        x=[ep for ep in range(1, epochs + 1)],\n",
        "        mode=\"lines\",\n",
        "        name=\"Recompensa\"\n",
        "    ),\n",
        "])\n",
        "fig.update_layout(\n",
        "    title=\"Recompensa por episódio\",\n",
        "    yaxis = dict(\n",
        "        title = \"Recompensa\"\n",
        "    ),\n",
        "    xaxis = dict(\n",
        "        title = \"Episódio\",\n",
        "    )\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VfKpGc4tQcp"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQl7m0h86PWZ"
      },
      "source": [
        "# Salva os dados (Tabela Q)\n",
        "pickle.dump(q, open(\"minicurso_rl/lab02/q_table.pkl\", \"wb\"))\n",
        "\n",
        "del q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUJk3HxxtOhr"
      },
      "source": [
        "# Carrega os dados salvos\n",
        "q = pickle.load(open(\"minicurso_rl/lab02/q_table.pkl\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQmSwRcf8Ziy"
      },
      "source": [
        "Finalmente, vamos testar o quão bem o Q-learning se saiu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmpNa5M-oBDF"
      },
      "source": [
        "frames = []\n",
        "episodes = 1\n",
        "done = False\n",
        "for episode in range(1, episodes+1):\n",
        "    r = 0\n",
        "    prev_state = env.reset()\n",
        "\n",
        "    while not done:\n",
        "        action = max(list(range(env.action_space.n)), key = lambda x: q[(prev_state,x)])\n",
        "        next_state, reward, done, _ = env.step(action)    \n",
        "\n",
        "        r += reward\n",
        "\n",
        "        frames.append({\n",
        "            \"frame\": env.render(mode=\"ansi\"),\n",
        "            \"state\": prev_state,\n",
        "            \"action\": action,\n",
        "            \"reward\": reward\n",
        "            }\n",
        "        )\n",
        "\n",
        "        prev_state = next_state\n",
        "\n",
        "    print(\"\\n\\nEpisódio: {} Pontuação: {}\".format(episode,r))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXpja63Vo2Qz"
      },
      "source": [
        "print_frames(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1gkYqgugBxs"
      },
      "source": [
        "del q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nF-Y-yjXe-q"
      },
      "source": [
        "#Deep Q Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGU5CpL-t38I"
      },
      "source": [
        "No Q-Learning, representamos o valor Q como uma tabela. No entanto, em muitos problemas do mundo real, teremos enormes espaços de estado e / ou ação e a representação tabular é insuficiente. Por exemplo, Computer Go tem $10^{170}$ estados possíveis e jogos como Breakout tem um espaço de estados contínuo (intensidade dos pixeis na tela). Quando é impossível armazenar todas as combinações possíveis de valores de estado e ação na matriz 2D ou tabela Q, precisamos usar Deep Q-Network (DQN) em vez do algoritmo Q-Learning.*\n",
        "\n",
        "*Em alguns casos é possível lidar com estados/ações contínuas agrupando esses valores em intervalos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtY8zOGXPR20"
      },
      "source": [
        "<img src=\"https://media.wired.com/photos/5955b2e6ad90646d424bb3cb/master/pass/GettyImages-475383836.jpg\" height=\"300\"> <img src=\"https://i.ytimg.com/vi/1oATuwG_dsA/hqdefault.jpg\" height=\"300\" width=\"250\">\n",
        "\n",
        "> Computer Go (esquerda) e Atari Breakout (direita)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7PPb04c-xqO"
      },
      "source": [
        "### Ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_dWUAqCt8Q8"
      },
      "source": [
        "O ambiente utilizado nesta parte será o [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/).\n",
        "\n",
        "A plataforma de pouso está sempre nas coordenadas (0,0). As coordenadas são os primeiros dois números no vetor de estado. A recompensa por pousar é de cerca de 100..140 pontos. Se o módulo de pouso se afastar da plataforma de pouso, ele perde a recompensa de volta. O episódio termina se o módulo de pouso bater ou parar, recebendo -100 ou +100 pontos adicionais. Cada contato de solo da perna é +10. O motor principal de disparo é -0,3 pontos por frame. Quatro ações discretas disponíveis: não fazer nada, disparar motor esquerdo, disparar motor principal, disparar motor direito.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Xwa3imfXrfo"
      },
      "source": [
        "# Procedimento para renderizar o ambiente no Google Colab\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt, animation\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "\n",
        "def create_anim(frames, dpi, fps):\n",
        "    plt.figure(figsize=(frames[0].shape[1] / dpi, frames[0].shape[0] / dpi), dpi=dpi)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    def setup():\n",
        "        plt.axis('off')\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, init_func=setup, frames=len(frames), interval=fps)\n",
        "    return anim\n",
        "\n",
        "def display_anim(frames, dpi=72, fps=60):\n",
        "    anim = create_anim(frames, dpi, fps)\n",
        "    return anim.to_jshtml()\n",
        "\n",
        "def save_anim(frames, filename, dpi=72, fps=50):\n",
        "    anim = create_anim(frames, dpi, fps)\n",
        "    anim.save(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgjHMigRTwmu"
      },
      "source": [
        "import gym\n",
        "\n",
        "environment_id = \"LunarLander-v2\"       # Nome do ambiente utilizado\n",
        "env = gym.make(environment_id)          # Criando o ambiente\n",
        "\n",
        "query_environment(environment_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krVT2SGxuDLv"
      },
      "source": [
        "frames = []\n",
        "episodes = 1\n",
        "for episode in range(1, episodes+1):\n",
        "    obs = env.reset()     # Retorna a observação inicial\n",
        "    done = False\n",
        "    score = 0\n",
        "    while not done:\n",
        "        frames.append(env.render(mode='rgb_array'))     # Renderizando o ambiente\n",
        "        action = env.action_space.sample()              # Seleciona uma ação aleatória\n",
        "        obs, reward, done, info = env.step(action)      # Executa a ação selecionada\n",
        "        score += reward\n",
        "    print(\"\\n\\nEpisódio: {} Pontuação: {}\".format(episode,score))\n",
        "env.close()\n",
        "\n",
        "display.HTML(display_anim(frames))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpmkixa-4Ue"
      },
      "source": [
        "### DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGi27fPwVjJH"
      },
      "source": [
        "#### Replay Buffer\n",
        "\n",
        "Iremos utilizar o ReplayBuffer para treinar nosso DQN. Ele armazena as transições que o agente observa, permitindo a reutilização desses dados posteriormente. Ao fazer a amostragem dele aleatoriamente, as transições que formam um batch são descorrelacionadas, ajudando a estabilizar e melhorar o treinamento do DQN.\n",
        "\n",
        "<img src=\"https://production-media.paperswithcode.com/methods/b6cdb8f5-ea3a-4cca-9331-f951c984d63a_MBK7MUl.png\" height=\"150\">\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rD5tf_0VWb7"
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, batch_size, seed):\n",
        "        \"\"\"\n",
        "        Replay memory permite que o agente registre experiências e aprenda \n",
        "        com elas.\n",
        "        \n",
        "        Parametros\n",
        "        ---------\n",
        "        buffer_size (int): tamanho máximo da memória interna\n",
        "        batch_size (int): tamanho do batch que será amostrado durante o treino\n",
        "        seed (int): random seed\n",
        "        \"\"\"\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.seed = random.seed(seed)\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Adicionar experiência\"\"\"\n",
        "        experience = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(experience)\n",
        "                \n",
        "    def sample(self):\n",
        "        \"\"\" \n",
        "        Amostrar aleatoriamente e retornar a tupla (estado, ação, recompensa, \n",
        "        próximo_estado, concluído) como torch tensors\n",
        "        \"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        \n",
        "        # Converter em torch tensors\n",
        "        states = torch.from_numpy(np.vstack([experience.state for experience in experiences if experience is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([experience.action for experience in experiences if experience is not None])).long().to(device)        \n",
        "        rewards = torch.from_numpy(np.vstack([experience.reward for experience in experiences if experience is not None])).float().to(device)        \n",
        "        next_states = torch.from_numpy(np.vstack([experience.next_state for experience in experiences if experience is not None])).float().to(device)  \n",
        "        \n",
        "        # Converter done de boolean para int\n",
        "        dones = torch.from_numpy(np.vstack([experience.done for experience in experiences if experience is not None]).astype(np.uint8)).float().to(device)        \n",
        "        \n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCsW7DEzWL_n"
      },
      "source": [
        "#### Algoritmo DQN\n",
        "\n",
        "Nosso objetivo será treinar uma política que tente maximizar a recompensa cumulativa com desconto\n",
        "\n",
        "$R_{t_0} = \\sum^{\\infty}_{t=t_0} \\gamma^{t-t_0}r_t$ \n",
        "\n",
        "Onde $R_{t_0}$ também é conhecido como retorno. O deconto, $\\gamma$, deve ser uma constante entre $0$ e $1$. Isso torna as recompensas de um futuro incerto e distante menos importante para nosso agente do que as recompensas de um futuro próximo em que ele pode estar bastante confiante.\n",
        "\n",
        "A ideia principal por trás do Q-learning é que se tivéssemos uma função $Q^*: Estado \\times Ação \\rightarrow \\mathbb{R}$ que poderia nos dizer qual seria nosso retorno, se tomássemos uma ação em um determinado estado, poderíamos facilmente construir uma política que maximizasse nossas recompensas:\n",
        "\n",
        "$\\pi^* (s) = \\arg \\max\\limits_{a} Q^* (s, a)$\n",
        "\n",
        "\n",
        "No entanto, não sabemos tudo sobre o mundo, por isso não temos acesso a $Q^*$. Mas, uma vez que as redes neurais são aproximadores universais de função, podemos simplesmente criar uma e treiná-la para se assemelhar $Q^*$.\n",
        "\n",
        "Para nossa regra de autalização, usaremos o fato de que cada função $Q$ para alguma política obedece à equação de Bellman:\n",
        "\n",
        "$Q^\\pi(s, a) = r + \\gamma Q^{\\pi}(s^{'}, \\pi(s^{'}))$\n",
        "\n",
        "\n",
        "A diferença entre os dois lados da igualdade é conhecida como erro de diferença temporal, $\\delta$:\n",
        "\n",
        "$\\delta = Q(s, a) - (r + \\gamma \\max\\limits_{a} Q^{\\pi}(s^{'}, a))$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQgFdTLIjwkJ"
      },
      "source": [
        "#### Q-Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQcNTUcgunSN"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed=0):\n",
        "        \"\"\"\n",
        "        Construa uma rede neural fully connected\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        state_size (int): Dimensão do estado\n",
        "        action_size (int): Dimensão da ação\n",
        "        seed (int): random seed\n",
        "        \"\"\"\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, 32)\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)  \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass\"\"\"\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "# Essa variável armazena o índice desta célula para facilitar a exportação do seu agente treinado.\n",
        "try: _q_cell = len(_ih) - 1\n",
        "except: pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYRb97vlmy5t"
      },
      "source": [
        "#### Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msIAqYYkESGi"
      },
      "source": [
        "Aqui você encontra a função ```learn``` que executa uma única etapa da otimização. Ele primeiro faz uma amostra de um batch, calcula $Q(s_t, a_t)$ e $V(s_{t+1}) = \\max\\limits_{a}Q(s_{t+1},a)$, e os combina na nossa loss. Também utilizamos uma Target Network fixa para calcular $V(s_{t+1})$ para uma maior estabilidade. A Target Network tem seus pesos mantidos congelados na maior parte do tempo, mas é atualizada com os pesos da rede Q de vez em quando.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUyuhzOlU36O"
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        \"\"\"\n",
        "        O Agente DQN interage com o ambiente, \n",
        "        armazena a experiência e aprende com ela\n",
        "        \n",
        "        Parametros\n",
        "        ----------\n",
        "        state_size (int): Dimensão do estado\n",
        "        action_size (int): Dimensão da ação\n",
        "        seed (int): random seed\n",
        "        \"\"\"\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        # Inicializar redes Q \n",
        "        self.q_network = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.fixed_network = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=LR)\n",
        "\n",
        "        # Inicializar memória\n",
        "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "        self.timestep = 0\n",
        "\n",
        "\n",
        "    # HW defina a função de loss\n",
        "    def criterion(self, predicted, target) -> torch.tensor:\n",
        "        \"\"\"\n",
        "        Calcula o erro quadrático médio entre o valor predito e o valor alvo\n",
        "\n",
        "        Parametros\n",
        "        ----------\n",
        "        predicted (torch.tensor): valor predito\n",
        "        target (torch.tensor): valor alvo\n",
        "\n",
        "        -> torch.Te\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "          \n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Atualiza o conhecimento do Agente\n",
        "        \n",
        "        Parametros\n",
        "        ----------\n",
        "        state (array_like): Estado atual do ambiente\n",
        "        action (int): Ação realizada no estado atual\n",
        "        reward (float): Recompensa recebida após a ação\n",
        "        next_state (array_like): Próximo estado retornado pelo ambiente após a ação\n",
        "        done (bool): se o episódio terminou após a ação\n",
        "        \"\"\"\n",
        "\n",
        "        # Salva transição no replay buffer\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        self.timestep += 1\n",
        "\n",
        "        # Realiza o aprendizado do agente a cada UPDATE_EVERY steps\n",
        "        if self.timestep % UPDATE_EVERY == 0:\n",
        "            if len(self.memory) > BATCH_SIZE:\n",
        "                sampled_experiences = self.memory.sample()\n",
        "                loss = self.learn(sampled_experiences)\n",
        "\n",
        "                return loss\n",
        "        return None\n",
        "        \n",
        "    def learn(self, experiences):\n",
        "        \"\"\"\n",
        "        Aprende com a experiência treinando a q_network\n",
        "        \n",
        "        Parametros\n",
        "        ----------\n",
        "        experiences (array_like): Lista de experiências amostradas da memória do agente\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Obtenha a ação com valor máximo de Q\n",
        "        action_values = self.fixed_network(next_states).detach()\n",
        "\n",
        "        # Notas\n",
        "        # tensor.max(1)[0] retorna os valores, tensor.max(1)[1] retorna os índices\n",
        "        # operação unsqueeze --> np.reshape\n",
        "        # Aqui nós fazemos torch.Size([64]) -> torch.Size([64, 1])\n",
        "        # O valor obtido será correspondente a max_a Q(S', a)\n",
        "        max_action_values = action_values.max(1)[0].unsqueeze(1)\n",
        "\n",
        "        # HW Defina o Q_target (Q*) com valores de ação com desconto para todas\n",
        "        # as transições não finais (done == 0). Para as transições finais (done == 1)\n",
        "        # Q(S', a) é 0, portanto utilize a recompensa diretamente.\n",
        "        # Q*(S, A) <- r + γ max_a Q(S', a)\n",
        "\n",
        "\n",
        "        # Calcula Q(s_t, a) - o modelo calcula Q (s_t), então selecionamos as \n",
        "        # colunas de ações tomadas. Estas são as ações que teriam sido tomadas \n",
        "        # para cada estado do batch de acordo com a rede Q\n",
        "        Q_predicted = self.q_network(states).gather(1, actions)\n",
        "        \n",
        "        # HW Calcula a loss\n",
        "\n",
        "        # HW zerar os gradientes\n",
        "\n",
        "        # HW calcula os novos gradientes (backward pass)\n",
        "\n",
        "        # HW atualiza os pesos com o otimizador\n",
        "        \n",
        "        # Atualizar pesos da rede Q fixa\n",
        "        self.update_fixed_network(self.q_network, self.fixed_network)\n",
        "\n",
        "        return loss.detach().cpu().numpy()\n",
        "        \n",
        "    def update_fixed_network(self, q_network, fixed_network):\n",
        "        \"\"\"\n",
        "        Atualize a rede fixa copiando os pesos da rede Q usando o parâmetro TAU\n",
        "        \n",
        "        Parametros\n",
        "        ----------\n",
        "        q_network (PyTorch model): Q network\n",
        "        fixed_network (PyTorch model): target network fixa\n",
        "        \"\"\"\n",
        "        for source_parameters, target_parameters in zip(q_network.parameters(), fixed_network.parameters()):\n",
        "            target_parameters.data.copy_(TAU * source_parameters.data + (1.0 - TAU) * target_parameters.data)\n",
        "        \n",
        "        \n",
        "    def act(self, state, eps=0.0):\n",
        "        \"\"\"\n",
        "        Escolha a ação\n",
        "        \n",
        "        Parametros\n",
        "        ----------\n",
        "        state (array_like): estado atual do ambiente\n",
        "        eps (float): epsilon para seleção epsilon-greedy de ação\n",
        "        \"\"\"\n",
        "        rnd = random.random()\n",
        "\n",
        "        if rnd < eps:\n",
        "            return np.random.randint(self.action_size)\n",
        "        else:\n",
        "            # Seleciona a melhor ação com probabilidade 1 - eps\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "            # coloque a rede em modo de avaliação\n",
        "            self.q_network.eval()\n",
        "            with torch.no_grad():\n",
        "                action_values = self.q_network(state)\n",
        "\n",
        "            # Voltar ao modo de treino\n",
        "            self.q_network.train()\n",
        "            action = np.argmax(action_values.cpu().data.numpy())\n",
        "            return action    \n",
        "        \n",
        "    def checkpoint(self, filename):\n",
        "        torch.save(self.q_network.state_dict(), filename)\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        self.q_network.load_state_dict(torch.load(filename))\n",
        "        self.fixed_network.load_state_dict(torch.load(filename))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cFleXOiVqkj"
      },
      "source": [
        "BUFFER_SIZE = int(1e5)  # Tamanho do Replay Buffer\n",
        "BATCH_SIZE = 64         # Número de experiências para amostrar da memória\n",
        "GAMMA = 0.99            # Fator de desconto\n",
        "TAU = 1e-3              # Parâmetro de atualização suave para atualização de rede Q fixa\n",
        "LR = 1e-4               # Taxa de aprendizagem\n",
        "UPDATE_EVERY = 4        # Com que frequência atualizar a rede Q \n",
        "\n",
        "MAX_EPISODES = 3000  # Número máximo de episódios para jogar\n",
        "MAX_STEPS = 1000     # passos máximos permitidos em um único episódio / jogo\n",
        "ENV_SOLVED = 200     # Pontuação MAX em que consideramos o ambiente ser resolvido\n",
        "PRINT_EVERY = 100    # Com que freqüência imprimir o progresso\n",
        "\n",
        "# Epsilon \n",
        "EPS_START = 1.0      # Valor padrão / inicial de eps\n",
        "EPS_DECAY = 0.999    # Taxa de decaimento do épsilon\n",
        "EPS_MIN = 0.01       # Épsilon mínimo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_aN9dtRB5gq"
      },
      "source": [
        "Antes de começar, vamos visualizar o efeito do decaimento do epsilon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgNbnsCA_7Vz"
      },
      "source": [
        "EPS_DECAY_RATES = [0.9, 0.99, 0.999, 0.9999]\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "fig = go.Figure([])\n",
        "\n",
        "_eps_list = []\n",
        "for decay_rate in EPS_DECAY_RATES:\n",
        "    test_eps = EPS_START\n",
        "    eps_list = []\n",
        "    for _ in range(MAX_EPISODES):\n",
        "        test_eps = max(test_eps * decay_rate, EPS_MIN)\n",
        "        eps_list.append(test_eps)          \n",
        "    \n",
        "    _eps_list.append(eps_list)\n",
        "\n",
        "\n",
        "fig = go.Figure([\n",
        "    go.Scatter(\n",
        "        y=_eps_list[0],\n",
        "        x=[ep for ep in range(1, len(eps_list) + 1)],\n",
        "        mode=\"lines\",\n",
        "        name=EPS_DECAY_RATES[0]\n",
        "    ),\n",
        "    go.Scatter(\n",
        "        y=_eps_list[1],\n",
        "        x=[ep for ep in range(1, len(eps_list) + 1)],\n",
        "        mode=\"lines\",\n",
        "        name=EPS_DECAY_RATES[1]\n",
        "    ),\n",
        "    go.Scatter(\n",
        "        y=_eps_list[2],\n",
        "        x=[ep for ep in range(1, len(eps_list) + 1)],\n",
        "        mode=\"lines\",\n",
        "        name=EPS_DECAY_RATES[2]\n",
        "    ),\n",
        "    go.Scatter(\n",
        "        y=_eps_list[3],\n",
        "        x=[ep for ep in range(1, len(eps_list) + 1)],\n",
        "        mode=\"lines\",\n",
        "        name=EPS_DECAY_RATES[3]\n",
        "    ),\n",
        "])\n",
        "fig.update_layout(\n",
        "    title=\"Efeito de várias taxas de decaimento\",\n",
        "    xaxis = dict(title=\"# Episódios\"),\n",
        "    yaxis = dict(title=\"Epsilon\")\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwS59LTJHBYc"
      },
      "source": [
        "Abaixo, você encontra o ciclo principal de treinamento. No início, redefinimos e inicializamos o ambiente. Em seguida, experimentamos uma ação, executamos, observamos o próximo estado e a recompensa e otimizamos nosso modelo uma vez. Quando o episódio termina, reiniciamos o loop. \n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1_RURPuJ9FcVM5RbHiIGRwAOSqXiMuPnw' width=\"620\" height=\"300\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr1XlIKECr8y"
      },
      "source": [
        "# Obtem tamanhos de estado e ação\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "print(\"Tamanho do estado: {}, tamanho da ação: {}\".format(state_size, action_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN6_RhslC4py"
      },
      "source": [
        "dqn_agent = DQNAgent(state_size, action_size, seed=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3RNNJlLC78g"
      },
      "source": [
        "from time import time\n",
        "\n",
        "start = time()\n",
        "scores = []\n",
        "loss = []\n",
        "\n",
        "# Mantem uma lista das últimas 100 pontuações\n",
        "scores_window = deque(maxlen=100)\n",
        "eps = EPS_START\n",
        "for episode in range(1, MAX_EPISODES + 1):\n",
        "    score = 0\n",
        "\n",
        "    # HW inicialize o ambiente e pegue a primeira observação\n",
        "    for t in range(MAX_STEPS):\n",
        "        # HW: Peça para o agente (dqn_agent) escolher uma ação dado a observação e \n",
        "        # um epsilon eps\n",
        "\n",
        "        # HW: Realize a ação no ambiente e receba o próximo estado, recompensa, done e info\n",
        "\n",
        "        # HW: Dado o estado, ação, recompensa, próximo_estado e done obtidos, \n",
        "        # atualize o conhecimento do agente, e guarde a _loss retornada para \n",
        "        # futuros logs\n",
        "\n",
        "        # HW: Atualize o estado anterior como próximo estado        \n",
        "\n",
        "        score += reward        \n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        if _loss is not None:\n",
        "            loss.append(_loss)\n",
        "            \n",
        "        eps = max(eps * EPS_DECAY, EPS_MIN)\n",
        "        if episode % PRINT_EVERY == 0:\n",
        "            mean_score = np.mean(scores_window)\n",
        "            print(\"\\r Progresso {}/{}, pontuação média:{:.2f}\".format(episode, MAX_EPISODES, mean_score), end=\"\")\n",
        "        if score >= ENV_SOLVED:\n",
        "            mean_score = np.mean(scores_window)\n",
        "            print(\"\\rAmbiente resolvido em {} episódios, pontuação média: {:.2f}\".format(episode, mean_score), end=\"\")\n",
        "            sys.stdout.flush()\n",
        "            dqn_agent.checkpoint(\"minicurso_rl/lab02/solved_dqn_ckpt.pth\")\n",
        "            break\n",
        "            \n",
        "    scores_window.append(score)\n",
        "    scores.append(score)\n",
        "    \n",
        "end = time()    \n",
        "print(\"{} segundos\".format(end - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUx7OughFqft"
      },
      "source": [
        "dqn_agent.checkpoint(\"minicurso_rl/lab02/last_dqn_ckpt.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzhx5Wm_iSaR"
      },
      "source": [
        "fig = go.Figure([\n",
        "    go.Scatter(\n",
        "        y=scores,\n",
        "        x=[ep for ep in range(1, len(scores) + 1)],\n",
        "        mode=\"lines\",\n",
        "        name=\"Recompensa\"\n",
        "    ),\n",
        "])\n",
        "fig.update_layout(\n",
        "    title=\"Recompensa\",\n",
        "    xaxis = dict(title=\"Episódios\")\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OAUyMb_u5Ts"
      },
      "source": [
        "fig = go.Figure([\n",
        "    go.Scatter(\n",
        "        y=loss,\n",
        "        x=[ep for ep in range(1, len(loss) + 1)],\n",
        "        mode=\"lines\",\n",
        "        name=\"Loss\"\n",
        "    ),\n",
        "])\n",
        "fig.update_layout(\n",
        "    title=\"Loss\",\n",
        "    xaxis = dict(title=\"Episódio\")\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfcOzXnYVCPY"
      },
      "source": [
        "Agora com o nosso agente já treinado, vamos ver a sua performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S66NWr85V0a1"
      },
      "source": [
        "dqn_agent = DQNAgent(state_size, action_size, seed=0)\n",
        "dqn_agent.load_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc6sjlsmU0WZ"
      },
      "source": [
        "frames = []\n",
        "episodes = 1\n",
        "for episode in range(1, episodes+1):\n",
        "    obs = env.reset()     \n",
        "    done = False\n",
        "    score = 0\n",
        "    while not done:\n",
        "        frames.append(env.render(mode='rgb_array'))     \n",
        "        action = dqn_agent.act(obs, 0)\n",
        "        obs, reward, done, info = env.step(action)      \n",
        "        score += reward\n",
        "    print(\"\\n\\nEpisódio: {} Pontuação: {}\".format(episode,score))\n",
        "env.close() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdJ_7jDXU5CM"
      },
      "source": [
        "display.HTML(display_anim(frames))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1duK5UFsJLT"
      },
      "source": [
        "# Bônus\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDeuxDo_lva9"
      },
      "source": [
        "Como vimos, o DQN é capaz de treinar uma política para atuar sobre um ambiente com observação contínua e ações discretas. Esse é o caso da variação `team_vs_policy` do ambiente `soccer_twos`, que será utilizado na competição final deste curso*.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/bryanoliveira/soccer-twos-env/master/images/screenshot.png\" height=\"400\">\n",
        "\n",
        "> Visualização do ambiente\n",
        "\n",
        "Este ambiente consiste em um jogo de futebol de carros 2x2, ou seja, o objetivo é marcar um gol no adversário o mais rápido possível. Na variação `team_vs_policy`, seu agente controla um jogador do time azul e joga contra um time aleatório. Mais informações sobre o ambiente podem ser encontradas [aqui](https://github.com/bryanoliveira/soccer-twos-env) e [aqui](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#soccer-twos).\n",
        "\n",
        "\n",
        "**Sua tarefa é treinar um agente com DQN para jogar contra esta política aleatória.**\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "*A variação utilizada na competição será a `multiagent_player`, mas agentes treinados para `team_vs_policy` podem ser facilmente adaptados. Na seção \"Exportando seu agente treinado\" o agente \"MyDqnSoccerAgent\" faz exatamente isso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDXqNr0rrQKj"
      },
      "source": [
        "Utilize o ambiente instanciado abaixo para executar o algoritmo de treinamento. Ao final da execução, a recompensa do seu agente por episódio deve tender a +2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwYmdXtUu2xi"
      },
      "source": [
        "import soccer_twos\n",
        "\n",
        "# Fecha o ambiente caso tenha sido aberto anteriormente\n",
        "try:\n",
        "    env.close()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "env = soccer_twos.make(variation=soccer_twos.EnvType.team_vs_policy, flatten_branched=True, single_player=True)\n",
        "\n",
        "# Obtem tamanhos de estado e ação\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "print(\"Tamanho do estado: {}, tamanho da ação: {}\".format(state_size, action_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIip_vIKcSkn"
      },
      "source": [
        "##### INSIRA AQUI O CÓDIGO PARA TREINO NO AMBIENTE DA COMPETIÇÃO\n",
        "\n",
        "# Dica: você só precisa do loop de coleta/treino e possivelmente tunar os hiperparâmetros.\n",
        "# Importante: esse ambiente não renderiza no Colab. Para visualizar seu agente você precisará exportá-lo para o seu computador."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfjQqywRcW0-"
      },
      "source": [
        "## Exportando seu agente treinado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HY_86MfcaCe"
      },
      "source": [
        "Após treinar seu agente com DQN, você pode exportá-lo para ser executado como competidor no ambiente da competição ou simplesmente assistí-lo. Para isso, devemos definir uma classe de agente que implemente a interface e trate as observações/ações para o formato da competição. Abaixo, guardamos a implementação em uma variável para salvá-la em um arquivo posteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijw4ChJRcY1e"
      },
      "source": [
        "agent_file = \"\"\"\n",
        "import os\n",
        "\n",
        "from gym_unity.envs import ActionFlattener\n",
        "import numpy as np\n",
        "import torch\n",
        "from soccer_twos import AgentInterface\n",
        "\n",
        "from .model import QNetwork\n",
        "\n",
        "\n",
        "class MyDqnSoccerAgent(AgentInterface):\n",
        "    def __init__(self, env):\n",
        "        # use flattened, Discrete actions instead of default MultiDiscrete\n",
        "        self.flattener = ActionFlattener(env.action_space.nvec)\n",
        "        # this agent's model works with team_vs_policy variation of the env\n",
        "        # so we need to convert observations & actions\n",
        "        self.model = QNetwork(env.observation_space.shape[0], self.flattener.action_space.n)\n",
        "        # load weights & put model in eval mode\n",
        "        self.model.load_state_dict(\n",
        "            torch.load(\n",
        "                os.path.join(\n",
        "                    os.path.dirname(os.path.abspath(__file__)), \"checkpoint.pth\"\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "        self.model.eval()\n",
        "\n",
        "    def act(self, observation):\n",
        "        actions = {}\n",
        "        # for each team player\n",
        "        for player_id in observation:\n",
        "            # create state tensor & feed it to model\n",
        "            state = torch.from_numpy(observation[player_id]).float().unsqueeze(0)\n",
        "            action_values = self.model(state)\n",
        "            action = np.argmax(action_values.data.numpy())\n",
        "            # convert Discrete action index to MultiDiscrete\n",
        "            actions[player_id] = self.flattener.lookup_action(action)\n",
        "        return actions\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE653HYQcku8"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "agent_path = \"minicurso_rl/lab02/my_dqn_soccer_agent/my_dqn_soccer_agent\"\n",
        "os.makedirs(agent_path, exist_ok=True)\n",
        "\n",
        "# salva os pesos do modelo\n",
        "dqn_agent.checkpoint(os.path.join(agent_path, \"checkpoint.pth\"))\n",
        "\n",
        "# salva a definição do modelo\n",
        "with open(os.path.join(agent_path, \"model.py\"), \"w\") as f:\n",
        "    f.write(_ih[_q_cell])\n",
        "\n",
        "# salva a classe do agente\n",
        "with open(os.path.join(agent_path, \"agent.py\"), \"w\") as f:\n",
        "    f.write(agent_file)\n",
        "\n",
        "# salva um __init__ para criar o módulo Python\n",
        "with open(os.path.join(agent_path, \"__init__.py\"), \"w\") as f:\n",
        "    f.write(\"from .agent import MyDqnSoccerAgent\")\n",
        "\n",
        "# empacota tudo num arquivo .zip\n",
        "shutil.make_archive(\"minicurso_rl/lab02/my_dqn_soccer_agent\", \"zip\", \"minicurso_rl/lab02/my_dqn_soccer_agent/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq7yHGBkcq6b"
      },
      "source": [
        "Após empacotar todos os arquivos necessários para a execução do seu agente, será criado um arquivo `minicurso_rl/lab02/my_dqn_soccer_agent.zip` nos arquivos do Colab e na pasta correspondente no Google Drive. Baixe o arquivo e extraia-o para alguma pasta no seu computador. \n",
        "\n",
        "Assumindo que o ambiente Python já está configurado (e.g. os pacotes no [requirements.txt](https://github.com/dlb-rl/rl-tournament-starter/blob/main/requirements.txt) estão instalados), rode `python -m soccer_twos.watch -m my_dqn_soccer_agent` para assistir seu agente jogando contra si mesmo. \n",
        "\n",
        "Você também pode testar dois agentes diferentes jogando um contra o outro. Utilize o seguinte comando: `python -m soccer_twos.watch -m1 my_dqn_soccer_agent -m2 ceia_baseline_agent`. Você pode baixar o agente *ceia_baseline_agent* [aqui](https://drive.google.com/file/d/1WEjr48D7QG9uVy1tf4GJAZTpimHtINzE/view).\n",
        "\n"
      ]
    }
  ]
}